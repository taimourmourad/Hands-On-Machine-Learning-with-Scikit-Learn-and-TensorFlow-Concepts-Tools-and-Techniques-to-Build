{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Now that we have built a character level model:\n    1. it's time to look at word-level models and tackle a common neural language processing task\n    2. Sentiment Analysis, \n    3. in this process we will learn how to handle sequences of variables length using masking","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport tensorflow_datasets as tfds","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:07.661114Z","iopub.execute_input":"2021-09-01T12:44:07.661576Z","iopub.status.idle":"2021-09-01T12:44:14.567673Z","shell.execute_reply.started":"2021-09-01T12:44:07.661537Z","shell.execute_reply":"2021-09-01T12:44:14.566488Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"X_train, consists of a list of reviews, each of which is represented as a numpy array:\n    1. where each integer represents a word\n    2. finally indexed by frequency(so low integers correspond to frequent words)\n    3. integers 0, 1, 2 are special, representing the padding token, start-of-sequence(SSS) token and Unknown words\n    4. 0 for negative, 1 for positive review","metadata":{}},{"cell_type":"code","source":"# Load the IMDb reviews\n(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\nX_train[0][:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:14.570033Z","iopub.execute_input":"2021-09-01T12:44:14.570560Z","iopub.status.idle":"2021-09-01T12:44:24.492689Z","shell.execute_reply.started":"2021-09-01T12:44:14.570507Z","shell.execute_reply":"2021-09-01T12:44:24.491688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n","output_type":"stream"},{"name":"stderr","text":"<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"},"metadata":{}}]},{"cell_type":"code","source":"# Google's SentencePiece project provides an Open Source Implementationo Descriped in a paper by Taku Kudo and John Richardson","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:24.494407Z","iopub.execute_input":"2021-09-01T12:44:24.494704Z","iopub.status.idle":"2021-09-01T12:44:24.500058Z","shell.execute_reply.started":"2021-09-01T12:44:24.494676Z","shell.execute_reply":"2021-09-01T12:44:24.499016Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# If you want to visualize a review, you can decode it like this\nword_index = keras.datasets.imdb.get_word_index()\nid_to_word = {id_ + 3: word for word, id_ in word_index.items()}# +3, because of 0, 1, and 3 tokens\nfor id_, token in enumerate(('<pad>', '<sos>', '<unk>')):# id_, 0, 1, 2\n    id_to_word[id_] = token","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:24.501730Z","iopub.execute_input":"2021-09-01T12:44:24.502018Z","iopub.status.idle":"2021-09-01T12:44:24.731745Z","shell.execute_reply.started":"2021-09-01T12:44:24.501991Z","shell.execute_reply":"2021-09-01T12:44:24.730595Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1646592/1641221 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"' '.join([id_to_word[id_] for id_ in X_train[0][:10]])","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:24.733042Z","iopub.execute_input":"2021-09-01T12:44:24.733367Z","iopub.status.idle":"2021-09-01T12:44:24.740487Z","shell.execute_reply.started":"2021-09-01T12:44:24.733338Z","shell.execute_reply":"2021-09-01T12:44:24.739488Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'<sos> this film was just brilliant casting location scenery story'"},"metadata":{}}]},{"cell_type":"markdown","source":"if you want to deploy your model to a mobile device or a web browser, and you don't want to write a different\npreprocessing function every time, then you will want to handle preprocessing using only Tensorflow operations:\n    1. first let's load the original IMDb reviews, as text(byte strings)\n    2. Using TensorFlow Datasets","metadata":{}},{"cell_type":"code","source":"# let's first load the original IMDb reviews, as text(byte strings)\n# Load the original IMDb reviews as text(byte strings), using Tensorflow Datasets(introduced in Chapter 13)\nimport tensorflow_datasets as tfds\n\ndatasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\ntrain_size = info.splits['train'].num_examples","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:44:24.743721Z","iopub.execute_input":"2021-09-01T12:44:24.744022Z","iopub.status.idle":"2021-09-01T12:45:28.380852Z","shell.execute_reply.started":"2021-09-01T12:44:24.743995Z","shell.execute_reply":"2021-09-01T12:45:28.379689Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713f08c2138249339cdd08fe1204c8c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b0c9dca8d84be6976efe3480fcaaaa"}},"metadata":{}},{"name":"stdout","text":"\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteCXL629/imdb_reviews-train.tfrecord\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460d0ee06489487ba9a2c98f26ccbe7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteCXL629/imdb_reviews-test.tfrecord\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f34c80f7aaf4875a1451bf132e7b4dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteCXL629/imdb_reviews-unsupervised.tfrecord\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3699bed3d64e43a99e90048320bd324c"}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"next:\n    1. it will use regular expression to replace <br />, tagw with spaces\n       for example: \"Well, i Can't<br />\" will become \"Well I Can't\"\nfinally:\n    1. the preprocess() function splits the reviews by the spaces, which returns a ragged tensor\n    2. and it converts the ragged tensor to a dense tensor, padding will reviews with the padding token \"<pad>\"\n       so that they all have the same length.","metadata":{}},{"cell_type":"code","source":"# Next, let's write the preprocessing function:\ndef preprocess(X_batch, y_batch):\n    X_batch = tf.strings.substr(X_batch, 0, 300)#truncating the review, keeps only the first 300 characters of each\n    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n    X_batch = tf.strings.split(X_batch)#which returns a ragged tensor\n    return X_batch.to_tensor(default_value=b'<pad>'), y_batch#it convert this ragged tensor to a dense tensor,padding all reviews with the padding token <pad>","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:28.382545Z","iopub.execute_input":"2021-09-01T12:45:28.382954Z","iopub.status.idle":"2021-09-01T12:45:28.389318Z","shell.execute_reply.started":"2021-09-01T12:45:28.382912Z","shell.execute_reply":"2021-09-01T12:45:28.388292Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Next, we need to construct the vocabulary.\n# this requires going through the whole training set once, applying our preprocess(), function \n# and using a counter to count the number of occurrences of each word.\nfrom collections import Counter\n\nvocabulary = Counter()\nfor X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n    for review in X_batch:\n        vocabulary.update(list(review.numpy()))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:28.392817Z","iopub.execute_input":"2021-09-01T12:45:28.393275Z","iopub.status.idle":"2021-09-01T12:45:33.741141Z","shell.execute_reply.started":"2021-09-01T12:45:28.393213Z","shell.execute_reply":"2021-09-01T12:45:33.740070Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# let's look at the 3 most common words:\nvocabulary.most_common()[:3]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.742709Z","iopub.execute_input":"2021-09-01T12:45:33.742999Z","iopub.status.idle":"2021-09-01T12:45:33.771632Z","shell.execute_reply.started":"2021-09-01T12:45:33.742972Z","shell.execute_reply":"2021-09-01T12:45:33.770355Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"},"metadata":{}}]},{"cell_type":"code","source":"# We don't need our model to know all the words in the dictionary to get good performance\n# so let's truncate the vocabulary, keeping only the 10,000 most common words:\nvocab_size = 10000\ntruncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.773335Z","iopub.execute_input":"2021-09-01T12:45:33.773780Z","iopub.status.idle":"2021-09-01T12:45:33.809313Z","shell.execute_reply.started":"2021-09-01T12:45:33.773737Z","shell.execute_reply":"2021-09-01T12:45:33.808082Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Now, we need to add a preprocessing step to replace each word with it's ID(it's index in the vocabulary)\n# just, like we did in Ch 13, we will create a lookup table for this, using 1000 out-of-vocabulary(oov) buckets\nwords = tf.constant(truncated_vocabulary)\nword_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\nvocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\nnum_oov_buckets = 1000\ntable = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.810722Z","iopub.execute_input":"2021-09-01T12:45:33.811020Z","iopub.status.idle":"2021-09-01T12:45:33.834207Z","shell.execute_reply.started":"2021-09-01T12:45:33.810992Z","shell.execute_reply":"2021-09-01T12:45:33.832863Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Note:\n    1. the words 'this', 'movie' and 'was', were found in the table so their IDs are lower than 10,000\n    2. while the word \"fantastic\", was not found, so it was mapped to one of the oov buckets, with an ID greater than or equal to 10,000\n","metadata":{}},{"cell_type":"code","source":"# We can then use this table to look up the IDs of a few words\ntable.lookup(tf.constant([b\"This movie was faaaaaaantastic\".split()]))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.835800Z","iopub.execute_input":"2021-09-01T12:45:33.836453Z","iopub.status.idle":"2021-09-01T12:45:33.866395Z","shell.execute_reply.started":"2021-09-01T12:45:33.836409Z","shell.execute_reply":"2021-09-01T12:45:33.865136Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10770]])>"},"metadata":{}}]},{"cell_type":"code","source":"# Now, we are ready to create the final training set.\n# We batch the reviews, then convert them to short sequences of words using the preprocess(), functionn\n# then encode, these words using encode_words() function that uses the table we just built, and finally prefetch the next batch\ndef encode_words(X_batch, y_batch):\n    return table.lookup(X_batch), y_batch\n\ntrain_set = datasets['train'].batch(32).map(preprocess)\ntrain_set = train_set.map(encode_words).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.868004Z","iopub.execute_input":"2021-09-01T12:45:33.868457Z","iopub.status.idle":"2021-09-01T12:45:33.963640Z","shell.execute_reply.started":"2021-09-01T12:45:33.868411Z","shell.execute_reply":"2021-09-01T12:45:33.961112Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# at last we can create the model and train it.\nembed_size = 128\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape = [None]),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.GRU(128),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\nhistory = model.fit(train_set, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:45:33.965775Z","iopub.execute_input":"2021-09-01T12:45:33.966342Z","iopub.status.idle":"2021-09-01T12:53:59.875930Z","shell.execute_reply.started":"2021-09-01T12:45:33.966289Z","shell.execute_reply":"2021-09-01T12:53:59.874681Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/5\n782/782 [==============================] - 104s 127ms/step - loss: 0.6886 - accuracy: 0.5266\nEpoch 2/5\n782/782 [==============================] - 102s 130ms/step - loss: 0.6240 - accuracy: 0.6435\nEpoch 3/5\n782/782 [==============================] - 100s 128ms/step - loss: 0.4911 - accuracy: 0.7554\nEpoch 4/5\n782/782 [==============================] - 99s 126ms/step - loss: 0.3855 - accuracy: 0.8270\nEpoch 5/5\n782/782 [==============================] - 101s 129ms/step - loss: 0.3146 - accuracy: 0.8679\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MASKING","metadata":{}},{"cell_type":"code","source":"K = keras.backend\ninputs = keras.layers.Input(shape=[None])\nmask= keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\nz = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\nz = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\nz = keras.layers.GRU(128)(z, mask=mask)\noutputs = keras.layers.Dense(1, activation='sigmoid')(z)\nmodel = keras.Model(inputs = [inputs], outputs=[outputs])","metadata":{"execution":{"iopub.status.busy":"2021-09-01T12:53:59.878482Z","iopub.execute_input":"2021-09-01T12:53:59.878840Z","iopub.status.idle":"2021-09-01T12:54:01.508913Z","shell.execute_reply.started":"2021-09-01T12:53:59.878805Z","shell.execute_reply":"2021-09-01T12:54:01.507867Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nAfter training for a few epochs, this model will become quite good at judging whether a review is positive or not\\n'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}