{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**let's lookt at a simple neural machine translation model that will translate English sentences to French**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the English sentences are fed to the Enocder:\n",
    "    1. note that the French translations are also used as inputs to the decoder but shifted back by one step\n",
    "    2. for the first very word it's given the  start-of-sequence(SOS) token\n",
    "    3. teh decoder is expected to end the sentence with an end of sequence(EOS) token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    1. that the English sentences are reversed before they are fed to the encoder\n",
    "    2. which is useful because that's generally the first thing that the decoder needs to translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Word is initially represented by it's 1D:\n",
    "    1. next an ebmedding layer returns the word embedding\n",
    "    2. these word embeddingss are what is actually fed to the encoder and the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at each step the decoder outputs a score for each word in the output vocabulary:\n",
    "    1. and then the softamx layer turns these scores into probabilities\n",
    "    2. the word with the highest probability is output\n",
    "    3. this is much like a regular classification task\n",
    "    4. so you can train the model using the 'sparse categorical crossetnropy' loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we have assumed that all input sequences have a constant length, but obviously sentences of the same length:\n",
    "    1. since regular tensors have fixed shapes, then can only contain sentences of the same length\n",
    "    2. you can use maksing to handle this\n",
    "    3. group sentences into buckets of similar length(bucket for the 1 to 6 word sentences)\n",
    "    4. another for the 7 to 12 word sentences and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to ignore any output past the EOS token:\n",
    "    1. so these tokens should not contribute to the loss(they must not masked out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the output Vocabulary is large(which is the case here):\n",
    "    1. outputing a probability for each and every possible word would be torribly slow\n",
    "    2. to avoid this one solution, is to look at the logits outputs by the model for the correct word and for a random sample of incorrect words\n",
    "    3. then compute an approximation of the loss based only on these logits\n",
    "    4. this sample softmax function can be used during training ans use the normal softmax function at inference time.\n",
    "    5. sampled softmax function can not be used at inference time because it requires knowing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "#LSTM, returns short and long term memory\n",
    "encoder = keras.layers.LSTM(512, return_state=True)#True,so that we can get the final hidden state and pass it to the decoder\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "#sampler is one of several samplers availabe in TensorFlow Addons,their role is to tell the decoder at each step what it\n",
    "# should pretend the previous output was , during inference\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 14s 312ms/step - loss: 4.6056\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 10s 298ms/step - loss: 4.6037\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
