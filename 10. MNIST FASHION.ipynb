{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Yourk Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First:\n",
    "    1. we need to load a dataset.\n",
    "    2. we will tackle the Fashion MNIST, which is a drop in replacement of MNIST.\n",
    "    3. but the images represent fashion items rather than handritten digits.\n",
    "    4. so each class is more diverse and the problem turns out to be significantly more challenging than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    1. the dataset is already split into a training set and a test set\n",
    "    2. there is no validation set, so let's craete one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover:\n",
    "    1. since we are going to train the neural network using Gradient Descent.\n",
    "    2. we must scale the input features.\n",
    "    3. for the simplicity, we just scale the pixel intensities down to the 0- 1, range by dividing themby 255,\n",
    "    4. this is also convert the floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trian, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train / 255.0, X_valid / 255.0 #X_valid, X_train, range from 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIth:\n",
    "    1. MNIST, when the label is equal to 5, it means that image represents the handwritten digit 5\n",
    "    2. However, for Fashion MNIST, we need the list of class names to know what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "    1. the first image in the training set response a coat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# display all the model's layers, None means batch size can be anything\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x12c88eefb38>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x12c88eefdd8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x12c8f306978>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x12c8f391908>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x12c88eefdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  fetch a layer by it's index\n",
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  fetch a layer by it's name\n",
    "model.get_layer('dense_2').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01330298,  0.01623482, -0.06464295, ..., -0.02638816,\n",
       "         0.02536875,  0.03286172],\n",
       "       [ 0.03042108, -0.03566369,  0.01817931, ..., -0.06447882,\n",
       "        -0.04660165,  0.01223708],\n",
       "       [-0.0241818 , -0.05938692,  0.04832375, ...,  0.00160716,\n",
       "         0.06783053, -0.01364114],\n",
       "       ...,\n",
       "       [-0.06256106,  0.05910307,  0.00945903, ..., -0.07301901,\n",
       "         0.00769576, -0.06264027],\n",
       "       [ 0.02287759,  0.05458528, -0.03168743, ..., -0.05016948,\n",
       "         0.0280839 , -0.0034865 ],\n",
       "       [-0.01046801,  0.0111837 , -0.06980243, ...,  0.07428327,\n",
       "        -0.01388569, -0.01479949]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is created:\n",
    "    1. you must call it's compile(), method to specify the loss-function and the optimizer to use\n",
    "    2. Optionally, you can also specify a list of extra metrics to compute during training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First:\n",
    "    1. we use the 'sparse_cagtegorical_crossentropy', loss\n",
    "    2. becuase, we have sparse labels(for each instance there is just a target class index, from 0 to 9 in this case)\n",
    "    3. and the classes are exclusive.\n",
    "    4. if instead we had one target probability per class for each instance(such as one-hot vectors)\n",
    "    5. then, we would need fication, (which is one, or more binary labels)\n",
    "    6. then, we would use the 'sigmoid', (i,e logistic) activation function in the output layer instead of the 'softmax' activation function\n",
    "    7. and we would use the 'binary_crossentropy', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally:\n",
    "    1. since this is a classifier,\n",
    "    2. it's useful to measure it's 'accuracy', during training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation the Mdel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras:\n",
    "    1. will measure the loss and the extra metrics on this set at the end of each epoch.\n",
    "    2. which is very useful set to see how well the model really performs\n",
    "    3. if the performance on the training set is much better on the validation set, your model is probably, # OVerfitting, the training set.\n",
    "    4. Or, there's a bug, such as a data mismatch between the training set and the validaltion set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2219 - accuracy: 0.9198 - val_loss: 0.2938 - val_accuracy: 0.8966\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2189 - accuracy: 0.9202 - val_loss: 0.2863 - val_accuracy: 0.8958\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2147 - accuracy: 0.9219 - val_loss: 0.2904 - val_accuracy: 0.8964\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2103 - accuracy: 0.9239 - val_loss: 0.3069 - val_accuracy: 0.8896\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2073 - accuracy: 0.9241 - val_loss: 0.2895 - val_accuracy: 0.8934\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2038 - accuracy: 0.9263 - val_loss: 0.2909 - val_accuracy: 0.8948\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2002 - accuracy: 0.9275 - val_loss: 0.2902 - val_accuracy: 0.8956\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1987 - accuracy: 0.9276 - val_loss: 0.2932 - val_accuracy: 0.8948\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1949 - accuracy: 0.9301 - val_loss: 0.3060 - val_accuracy: 0.8910\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1904 - accuracy: 0.9314 - val_loss: 0.2996 - val_accuracy: 0.8924\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1886 - accuracy: 0.9309 - val_loss: 0.2886 - val_accuracy: 0.8950\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1852 - accuracy: 0.9335 - val_loss: 0.2855 - val_accuracy: 0.8972\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1823 - accuracy: 0.9343 - val_loss: 0.3057 - val_accuracy: 0.8926\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1797 - accuracy: 0.9346 - val_loss: 0.2998 - val_accuracy: 0.8920\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1764 - accuracy: 0.9371 - val_loss: 0.3070 - val_accuracy: 0.8880\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1740 - accuracy: 0.9380 - val_loss: 0.3018 - val_accuracy: 0.8948\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1713 - accuracy: 0.9379 - val_loss: 0.3018 - val_accuracy: 0.8970\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1685 - accuracy: 0.9392 - val_loss: 0.3133 - val_accuracy: 0.8860\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1653 - accuracy: 0.9402 - val_loss: 0.3085 - val_accuracy: 0.8936\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1623 - accuracy: 0.9417 - val_loss: 0.2889 - val_accuracy: 0.8970\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1608 - accuracy: 0.9430 - val_loss: 0.2934 - val_accuracy: 0.8974\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1577 - accuracy: 0.9434 - val_loss: 0.2890 - val_accuracy: 0.8990\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1554 - accuracy: 0.9445 - val_loss: 0.3132 - val_accuracy: 0.8922\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1530 - accuracy: 0.9453 - val_loss: 0.2964 - val_accuracy: 0.8992\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1501 - accuracy: 0.9464 - val_loss: 0.3092 - val_accuracy: 0.8928\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1470 - accuracy: 0.9478 - val_loss: 0.2942 - val_accuracy: 0.8962\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1453 - accuracy: 0.9483 - val_loss: 0.3066 - val_accuracy: 0.8912\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1422 - accuracy: 0.9495 - val_loss: 0.3052 - val_accuracy: 0.8922\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1412 - accuracy: 0.9502 - val_loss: 0.3002 - val_accuracy: 0.9006\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1398 - accuracy: 0.9505 - val_loss: 0.3136 - val_accuracy: 0.8970\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see:\n",
    "    1. that the training  loss went down, which is a good sign\n",
    "    2. and the validation accuracy is, 0.8952, after 50 epochs, not too far from the training accuracy, so there doesn't seem to be much overfitting going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the training set way very skewed:\n",
    "    1. with some classes being overrepresented and others underrepresented,\n",
    "    2. it would be useful to set the, class_weight, argument when calling the, fit(). method.\n",
    "    3. giving a larger weight to underrepresented classes and a lower wight to overrepresented classes.\n",
    "    4. These weights would be used by keras when computing the, loss.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need:\n",
    "    1. pre-instance weights, instead, you can set the, sample_weight argument(it supersedes, class_weight)\n",
    "    2. This could be useful for example if some instances were labeled by experts while others were labeled using a crowdsourcing platform:\n",
    "        1. you might want to give more weight to the former.\n",
    "        2. yoy can also provide sample weights(but not class weights), for the validation set by adding them as a third item inn the validation_data, tuplem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 30, 'steps': 1719}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the fit(), method:\n",
    "    1.returns a Histroy, object containing the training parameters(history.params)\n",
    "    2. the list of, epochs it went through(history.epoch)\n",
    "    3. and the most importantly a dictionary(history.history), containing the loss and extra metrics ti measured at the end of each epoch on the training set and on the validation set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.22189372777938843,\n",
       "  0.21888545155525208,\n",
       "  0.21473631262779236,\n",
       "  0.2103496938943863,\n",
       "  0.2072809338569641,\n",
       "  0.20382043719291687,\n",
       "  0.20020060241222382,\n",
       "  0.1987149715423584,\n",
       "  0.19488190114498138,\n",
       "  0.19037964940071106,\n",
       "  0.18856659531593323,\n",
       "  0.18518243730068207,\n",
       "  0.18229472637176514,\n",
       "  0.17966434359550476,\n",
       "  0.17639203369617462,\n",
       "  0.17401063442230225,\n",
       "  0.17131240665912628,\n",
       "  0.1684844046831131,\n",
       "  0.16532902419567108,\n",
       "  0.16234396398067474,\n",
       "  0.1608302742242813,\n",
       "  0.15766488015651703,\n",
       "  0.1554463654756546,\n",
       "  0.15301232039928436,\n",
       "  0.15006396174430847,\n",
       "  0.14703308045864105,\n",
       "  0.14527639746665955,\n",
       "  0.14224383234977722,\n",
       "  0.14121082425117493,\n",
       "  0.13975737988948822],\n",
       " 'accuracy': [0.9197999835014343,\n",
       "  0.920236349105835,\n",
       "  0.921854555606842,\n",
       "  0.9238545298576355,\n",
       "  0.9240545630455017,\n",
       "  0.9262909293174744,\n",
       "  0.927545428276062,\n",
       "  0.9276182055473328,\n",
       "  0.9300909042358398,\n",
       "  0.9314000010490417,\n",
       "  0.9309090971946716,\n",
       "  0.9334545731544495,\n",
       "  0.9343090653419495,\n",
       "  0.9346181750297546,\n",
       "  0.93707275390625,\n",
       "  0.9380000233650208,\n",
       "  0.9378727078437805,\n",
       "  0.9392363429069519,\n",
       "  0.9401817917823792,\n",
       "  0.9416909217834473,\n",
       "  0.9429636597633362,\n",
       "  0.9434182047843933,\n",
       "  0.944490909576416,\n",
       "  0.9452545642852783,\n",
       "  0.946363627910614,\n",
       "  0.9477999806404114,\n",
       "  0.948272705078125,\n",
       "  0.9494909048080444,\n",
       "  0.9502363801002502,\n",
       "  0.9505272507667542],\n",
       " 'val_loss': [0.29380276799201965,\n",
       "  0.2862911522388458,\n",
       "  0.29039266705513,\n",
       "  0.30686360597610474,\n",
       "  0.289485901594162,\n",
       "  0.29091110825538635,\n",
       "  0.2902126610279083,\n",
       "  0.2931922674179077,\n",
       "  0.3060019612312317,\n",
       "  0.29957935214042664,\n",
       "  0.2885710597038269,\n",
       "  0.2854692339897156,\n",
       "  0.3057476580142975,\n",
       "  0.29980868101119995,\n",
       "  0.30704984068870544,\n",
       "  0.3017876148223877,\n",
       "  0.30176547169685364,\n",
       "  0.3133058249950409,\n",
       "  0.3085409700870514,\n",
       "  0.2888545095920563,\n",
       "  0.2934206426143646,\n",
       "  0.28897419571876526,\n",
       "  0.31321993470191956,\n",
       "  0.2963515520095825,\n",
       "  0.30917784571647644,\n",
       "  0.29418694972991943,\n",
       "  0.30662423372268677,\n",
       "  0.3052118718624115,\n",
       "  0.3001924455165863,\n",
       "  0.3135942220687866],\n",
       " 'val_accuracy': [0.8966000080108643,\n",
       "  0.895799994468689,\n",
       "  0.896399974822998,\n",
       "  0.8895999789237976,\n",
       "  0.8934000134468079,\n",
       "  0.8948000073432922,\n",
       "  0.8956000208854675,\n",
       "  0.8948000073432922,\n",
       "  0.890999972820282,\n",
       "  0.8924000263214111,\n",
       "  0.8949999809265137,\n",
       "  0.8971999883651733,\n",
       "  0.8925999999046326,\n",
       "  0.8920000195503235,\n",
       "  0.8880000114440918,\n",
       "  0.8948000073432922,\n",
       "  0.8970000147819519,\n",
       "  0.8859999775886536,\n",
       "  0.8935999870300293,\n",
       "  0.8970000147819519,\n",
       "  0.8974000215530396,\n",
       "  0.8989999890327454,\n",
       "  0.8921999931335449,\n",
       "  0.8992000222206116,\n",
       "  0.892799973487854,\n",
       "  0.8962000012397766,\n",
       "  0.8912000060081482,\n",
       "  0.8921999931335449,\n",
       "  0.900600016117096,\n",
       "  0.8970000147819519]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learing Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)# Set the vertival range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see:\n",
    "    1. The validation curves are quite close to the training curves, which means that there is not too much overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not satisfied with the performance of your model:\n",
    "    1. you should go back and tune the model's hyperparameters\n",
    "    2. for example, the number of layers\n",
    "    3. the number of neurons per layer\n",
    "    4. the types of activation functions we use for each hidden layer.\n",
    "    5. the number of training ebochs\n",
    "    6. the batch size( it can be set in the fit() method using the batch-size, argument. which default to 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once You are satisfied with your model's validationn accuracy:\n",
    "    1. you should evaluate it on the test set to estimate the generalization error before you deploy the model to production.\n",
    "    w. you can easily do this using the, evaluate(), method(it's supports several othe arguments, such as batch_size, or sample wieght, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 68.3327 - accuracy: 0.8603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[68.33267974853516, 0.8603000044822693]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in CH2:\n",
    "    1. it's common to get slightly lower performance on the test set than the valiation set.\n",
    "    2. because the hyperparameters are tunes on the validation set, not the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model to Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only care about:\n",
    "    1. the class, with the highest estimation probability.\n",
    "    2. even if that probability is quite low\n",
    "    3. then you can use teh predict_classes(), method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taimo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shirt', 'Shirt', 'Shirt'], dtype='<U11')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but also save checkpoints at regular intervals during training by default at the end of each epoch\n",
    "    <br>\n",
    "this way you don't need to worry about training for too long and overfitting the training set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('2. Building an Image Classifier Using the Sequential API.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3084 - accuracy: 0.1026 - val_loss: 2.2791 - val_accuracy: 0.0999\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3105 - accuracy: 0.1030 - val_loss: 2.2382 - val_accuracy: 0.2331\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.1036 - val_loss: 2.2500 - val_accuracy: 0.1004\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3106 - accuracy: 0.0960 - val_loss: 2.2420 - val_accuracy: 0.2917\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3105 - accuracy: 0.0900 - val_loss: 2.2420 - val_accuracy: 0.2296\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3089 - accuracy: 0.0970 - val_loss: 2.2473 - val_accuracy: 0.1030\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3108 - accuracy: 0.1008 - val_loss: 2.2437 - val_accuracy: 0.3471\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3097 - accuracy: 0.0934 - val_loss: 2.2380 - val_accuracy: 0.1983\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3113 - accuracy: 0.0912 - val_loss: 2.2508 - val_accuracy: 0.2581\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3104 - accuracy: 0.0984 - val_loss: 2.2593 - val_accuracy: 0.0999\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.1034 - val_loss: 2.2467 - val_accuracy: 0.1210\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.1000 - val_loss: 2.2483 - val_accuracy: 0.1011\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3112 - accuracy: 0.0980 - val_loss: 2.2488 - val_accuracy: 0.2270\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3073 - accuracy: 0.1038 - val_loss: 2.2531 - val_accuracy: 0.0999\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3103 - accuracy: 0.0996 - val_loss: 2.2476 - val_accuracy: 0.0989\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3095 - accuracy: 0.0994 - val_loss: 2.2396 - val_accuracy: 0.1531\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3108 - accuracy: 0.0948 - val_loss: 2.2373 - val_accuracy: 0.2077\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3106 - accuracy: 0.0984 - val_loss: 2.2406 - val_accuracy: 0.1547\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3097 - accuracy: 0.0968 - val_loss: 2.2436 - val_accuracy: 0.2047\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.1030 - val_loss: 2.2602 - val_accuracy: 0.2071\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.1044 - val_loss: 2.2387 - val_accuracy: 0.1750\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3087 - accuracy: 0.0986 - val_loss: 2.2471 - val_accuracy: 0.1681\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3097 - accuracy: 0.1038 - val_loss: 2.2429 - val_accuracy: 0.2426\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3078 - accuracy: 0.1052 - val_loss: 2.2415 - val_accuracy: 0.3440\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.1034 - val_loss: 2.2484 - val_accuracy: 0.1899\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3104 - accuracy: 0.1024 - val_loss: 2.2447 - val_accuracy: 0.2153\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0988 - val_loss: 2.2395 - val_accuracy: 0.2246\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0958 - val_loss: 2.2449 - val_accuracy: 0.2691\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3084 - accuracy: 0.0994 - val_loss: 2.2412 - val_accuracy: 0.1820\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3087 - accuracy: 0.0996 - val_loss: 2.2612 - val_accuracy: 0.0996\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3097 - accuracy: 0.1032 - val_loss: 2.2452 - val_accuracy: 0.2013\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3105 - accuracy: 0.0954 - val_loss: 2.2430 - val_accuracy: 0.1003\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.0956 - val_loss: 2.2471 - val_accuracy: 0.2037\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0974 - val_loss: 2.2543 - val_accuracy: 0.1017\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.0976 - val_loss: 2.2413 - val_accuracy: 0.2636\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3108 - accuracy: 0.1000 - val_loss: 2.2410 - val_accuracy: 0.2881\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.0958 - val_loss: 2.2425 - val_accuracy: 0.1117\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3077 - accuracy: 0.1060 - val_loss: 2.2456 - val_accuracy: 0.1201\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0956 - val_loss: 2.2471 - val_accuracy: 0.0999\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3086 - accuracy: 0.1078 - val_loss: 2.2492 - val_accuracy: 0.2147\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0994 - val_loss: 2.2479 - val_accuracy: 0.2451\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.1000 - val_loss: 2.2456 - val_accuracy: 0.2146\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.0892 - val_loss: 2.2536 - val_accuracy: 0.0977\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.0958 - val_loss: 2.2395 - val_accuracy: 0.4589\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0994 - val_loss: 2.2431 - val_accuracy: 0.1214\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3100 - accuracy: 0.0944 - val_loss: 2.2430 - val_accuracy: 0.2279\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3107 - accuracy: 0.0952 - val_loss: 2.2425 - val_accuracy: 0.1541\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.1004 - val_loss: 2.2486 - val_accuracy: 0.3271\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0962 - val_loss: 2.2453 - val_accuracy: 0.2181\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3088 - accuracy: 0.0958 - val_loss: 2.2410 - val_accuracy: 0.1113\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0994 - val_loss: 2.2529 - val_accuracy: 0.2376\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3077 - accuracy: 0.1042 - val_loss: 2.2421 - val_accuracy: 0.1199\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.0990 - val_loss: 2.2419 - val_accuracy: 0.3227\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3083 - accuracy: 0.1024 - val_loss: 2.2590 - val_accuracy: 0.0999\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0936 - val_loss: 2.2487 - val_accuracy: 0.1443\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1012 - val_loss: 2.2466 - val_accuracy: 0.1640\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3101 - accuracy: 0.0900 - val_loss: 2.2520 - val_accuracy: 0.2247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0908 - val_loss: 2.2396 - val_accuracy: 0.3080\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3098 - accuracy: 0.0990 - val_loss: 2.2422 - val_accuracy: 0.2906\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.1054 - val_loss: 2.2439 - val_accuracy: 0.2111\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3059 - accuracy: 0.1096 - val_loss: 2.2481 - val_accuracy: 0.1319\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3095 - accuracy: 0.0982 - val_loss: 2.2412 - val_accuracy: 0.3024\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.1016 - val_loss: 2.2417 - val_accuracy: 0.1887\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1000 - val_loss: 2.2447 - val_accuracy: 0.1374\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0932 - val_loss: 2.2485 - val_accuracy: 0.2810\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3086 - accuracy: 0.1012 - val_loss: 2.2394 - val_accuracy: 0.4500\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.0966 - val_loss: 2.2480 - val_accuracy: 0.2579\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.0962 - val_loss: 2.2459 - val_accuracy: 0.2904\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3083 - accuracy: 0.1076 - val_loss: 2.2658 - val_accuracy: 0.1006\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3101 - accuracy: 0.0968 - val_loss: 2.2458 - val_accuracy: 0.1437\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3089 - accuracy: 0.0956 - val_loss: 2.2488 - val_accuracy: 0.0974\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3076 - accuracy: 0.0990 - val_loss: 2.2440 - val_accuracy: 0.1116\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3091 - accuracy: 0.0982 - val_loss: 2.2485 - val_accuracy: 0.1746\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0962 - val_loss: 2.2416 - val_accuracy: 0.3161\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3083 - accuracy: 0.1004 - val_loss: 2.2501 - val_accuracy: 0.0974\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3086 - accuracy: 0.1040 - val_loss: 2.2531 - val_accuracy: 0.2716\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3085 - accuracy: 0.1014 - val_loss: 2.2437 - val_accuracy: 0.2080\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3082 - accuracy: 0.1020 - val_loss: 2.2439 - val_accuracy: 0.3089\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3090 - accuracy: 0.1016 - val_loss: 2.2518 - val_accuracy: 0.1193\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3078 - accuracy: 0.0980 - val_loss: 2.2455 - val_accuracy: 0.2650\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.0960 - val_loss: 2.2532 - val_accuracy: 0.0999\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3081 - accuracy: 0.1018 - val_loss: 2.2454 - val_accuracy: 0.1089\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.1014 - val_loss: 2.2469 - val_accuracy: 0.1451\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3088 - accuracy: 0.0980 - val_loss: 2.2419 - val_accuracy: 0.2340\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3084 - accuracy: 0.0988 - val_loss: 2.2418 - val_accuracy: 0.2131\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3086 - accuracy: 0.0960 - val_loss: 2.2482 - val_accuracy: 0.0990\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3071 - accuracy: 0.1086 - val_loss: 2.2424 - val_accuracy: 0.2003\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.0992 - val_loss: 2.2546 - val_accuracy: 0.1393\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3070 - accuracy: 0.1082 - val_loss: 2.2446 - val_accuracy: 0.0999\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3095 - accuracy: 0.0924 - val_loss: 2.2495 - val_accuracy: 0.1003\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3083 - accuracy: 0.0994 - val_loss: 2.2418 - val_accuracy: 0.3311\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0936 - val_loss: 2.2450 - val_accuracy: 0.3041\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0958 - val_loss: 2.2462 - val_accuracy: 0.1223\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3085 - accuracy: 0.1018 - val_loss: 2.2471 - val_accuracy: 0.1844\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3087 - accuracy: 0.0942 - val_loss: 2.2441 - val_accuracy: 0.2250\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.0988 - val_loss: 2.2399 - val_accuracy: 0.4617\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3078 - accuracy: 0.1046 - val_loss: 2.2465 - val_accuracy: 0.1683\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3075 - accuracy: 0.0988 - val_loss: 2.2427 - val_accuracy: 0.1840\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3088 - accuracy: 0.0936 - val_loss: 2.2405 - val_accuracy: 0.2350\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3088 - accuracy: 0.0972 - val_loss: 2.2476 - val_accuracy: 0.1770\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"2. Building an Image Classifier Using the Sequential API.h5\", save_best_only=True, patience=10)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model('2. Building an Image Classifier Using the Sequential API.h5',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fune Tuning NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3094 - accuracy: 0.1000 - val_loss: 2.2428 - val_accuracy: 0.1771\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.1006 - val_loss: 2.2423 - val_accuracy: 0.2833\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.0960 - val_loss: 2.2454 - val_accuracy: 0.0990\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3101 - accuracy: 0.0970 - val_loss: 2.2503 - val_accuracy: 0.1026\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.1000 - val_loss: 2.2635 - val_accuracy: 0.1004\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.1074 - val_loss: 2.2519 - val_accuracy: 0.0999\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0972 - val_loss: 2.2467 - val_accuracy: 0.2034\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3105 - accuracy: 0.0986 - val_loss: 2.2572 - val_accuracy: 0.1011\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3098 - accuracy: 0.1010 - val_loss: 2.2507 - val_accuracy: 0.2786\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0992 - val_loss: 2.2418 - val_accuracy: 0.3067\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.0970 - val_loss: 2.2510 - val_accuracy: 0.2451\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3087 - accuracy: 0.1030 - val_loss: 2.2616 - val_accuracy: 0.1026\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.1000 - val_loss: 2.2526 - val_accuracy: 0.2844\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.1024 - val_loss: 2.2411 - val_accuracy: 0.4190\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3099 - accuracy: 0.0976 - val_loss: 2.2398 - val_accuracy: 0.4296\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3099 - accuracy: 0.0994 - val_loss: 2.2452 - val_accuracy: 0.1876\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3103 - accuracy: 0.0986 - val_loss: 2.2419 - val_accuracy: 0.1884\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.0966 - val_loss: 2.2436 - val_accuracy: 0.1071\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.1048 - val_loss: 2.2397 - val_accuracy: 0.2367\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3100 - accuracy: 0.0998 - val_loss: 2.2415 - val_accuracy: 0.4206\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3099 - accuracy: 0.1034 - val_loss: 2.2432 - val_accuracy: 0.0984\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3074 - accuracy: 0.1078 - val_loss: 2.2538 - val_accuracy: 0.2154\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.0970 - val_loss: 2.2387 - val_accuracy: 0.2673\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0972 - val_loss: 2.2411 - val_accuracy: 0.1177\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.1032 - val_loss: 2.2523 - val_accuracy: 0.2171\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.0962 - val_loss: 2.2601 - val_accuracy: 0.0999\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1002 - val_loss: 2.2422 - val_accuracy: 0.1933\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3071 - accuracy: 0.1036 - val_loss: 2.2499 - val_accuracy: 0.1014\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.0990 - val_loss: 2.2442 - val_accuracy: 0.0999\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0996 - val_loss: 2.2423 - val_accuracy: 0.2447\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3098 - accuracy: 0.0920 - val_loss: 2.2444 - val_accuracy: 0.1423\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0980 - val_loss: 2.2490 - val_accuracy: 0.1070\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3085 - accuracy: 0.1044 - val_loss: 2.2440 - val_accuracy: 0.1014\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3102 - accuracy: 0.0950 - val_loss: 2.2508 - val_accuracy: 0.1810\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3096 - accuracy: 0.0934 - val_loss: 2.2410 - val_accuracy: 0.1874\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.1052 - val_loss: 2.2420 - val_accuracy: 0.3437\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1006 - val_loss: 2.2616 - val_accuracy: 0.0999\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3106 - accuracy: 0.0938 - val_loss: 2.2394 - val_accuracy: 0.2653\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0956 - val_loss: 2.2468 - val_accuracy: 0.2599\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0990 - val_loss: 2.2470 - val_accuracy: 0.2803\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3096 - accuracy: 0.0964 - val_loss: 2.2566 - val_accuracy: 0.1254\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3097 - accuracy: 0.0936 - val_loss: 2.2442 - val_accuracy: 0.3174\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3086 - accuracy: 0.0936 - val_loss: 2.2409 - val_accuracy: 0.2596\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.1006 - val_loss: 2.2582 - val_accuracy: 0.1296\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3098 - accuracy: 0.0980 - val_loss: 2.2473 - val_accuracy: 0.2011\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3066 - accuracy: 0.0998 - val_loss: 2.2482 - val_accuracy: 0.1920\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.1002 - val_loss: 2.2444 - val_accuracy: 0.1031\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.1012 - val_loss: 2.2394 - val_accuracy: 0.2947\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0926 - val_loss: 2.2690 - val_accuracy: 0.1907\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.1022 - val_loss: 2.2430 - val_accuracy: 0.1737\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0984 - val_loss: 2.2447 - val_accuracy: 0.1520\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3091 - accuracy: 0.0972 - val_loss: 2.2423 - val_accuracy: 0.3844\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3098 - accuracy: 0.0882 - val_loss: 2.2489 - val_accuracy: 0.2119\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3092 - accuracy: 0.0974 - val_loss: 2.2439 - val_accuracy: 0.2023\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3087 - accuracy: 0.1006 - val_loss: 2.2428 - val_accuracy: 0.1714\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3078 - accuracy: 0.1052 - val_loss: 2.2490 - val_accuracy: 0.0999\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 2.3079 - accuracy: 0.1016 - val_loss: 2.2524 - val_accuracy: 0.0999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3086 - accuracy: 0.1022 - val_loss: 2.2507 - val_accuracy: 0.3014\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0906 - val_loss: 2.2410 - val_accuracy: 0.2801\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.1002 - val_loss: 2.2450 - val_accuracy: 0.1580\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.0984 - val_loss: 2.2448 - val_accuracy: 0.1860\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0972 - val_loss: 2.2421 - val_accuracy: 0.2277\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1066 - val_loss: 2.2488 - val_accuracy: 0.2011\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.1006 - val_loss: 2.2439 - val_accuracy: 0.1094\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3068 - accuracy: 0.1020 - val_loss: 2.2488 - val_accuracy: 0.1909\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0928 - val_loss: 2.2478 - val_accuracy: 0.1137\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3085 - accuracy: 0.0962 - val_loss: 2.2486 - val_accuracy: 0.0999\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1000 - val_loss: 2.2421 - val_accuracy: 0.1309\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3074 - accuracy: 0.0982 - val_loss: 2.2495 - val_accuracy: 0.1511\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.0948 - val_loss: 2.2449 - val_accuracy: 0.1011\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3073 - accuracy: 0.0986 - val_loss: 2.2456 - val_accuracy: 0.2313\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3084 - accuracy: 0.0988 - val_loss: 2.2442 - val_accuracy: 0.2250\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.1010 - val_loss: 2.2441 - val_accuracy: 0.2320\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3077 - accuracy: 0.1028 - val_loss: 2.2472 - val_accuracy: 0.1324\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3078 - accuracy: 0.1002 - val_loss: 2.2567 - val_accuracy: 0.1814\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0974 - val_loss: 2.2545 - val_accuracy: 0.0999\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3096 - accuracy: 0.0936 - val_loss: 2.2437 - val_accuracy: 0.2227\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0934 - val_loss: 2.2470 - val_accuracy: 0.2030\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3083 - accuracy: 0.0984 - val_loss: 2.2466 - val_accuracy: 0.2066\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3094 - accuracy: 0.0902 - val_loss: 2.2391 - val_accuracy: 0.4634\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0942 - val_loss: 2.2474 - val_accuracy: 0.1331\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.1038 - val_loss: 2.2399 - val_accuracy: 0.3637\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3090 - accuracy: 0.0932 - val_loss: 2.2494 - val_accuracy: 0.0973\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0922 - val_loss: 2.2516 - val_accuracy: 0.2934\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3073 - accuracy: 0.1042 - val_loss: 2.2557 - val_accuracy: 0.1947\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3082 - accuracy: 0.1002 - val_loss: 2.2504 - val_accuracy: 0.1686\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3084 - accuracy: 0.1002 - val_loss: 2.2502 - val_accuracy: 0.1633\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.1042 - val_loss: 2.2417 - val_accuracy: 0.1153\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3089 - accuracy: 0.0986 - val_loss: 2.2469 - val_accuracy: 0.3466\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.0978 - val_loss: 2.2515 - val_accuracy: 0.0990\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3088 - accuracy: 0.0942 - val_loss: 2.2456 - val_accuracy: 0.1270\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.0970 - val_loss: 2.2468 - val_accuracy: 0.2207\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3077 - accuracy: 0.0986 - val_loss: 2.2564 - val_accuracy: 0.1011\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.1008 - val_loss: 2.2500 - val_accuracy: 0.1060\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3079 - accuracy: 0.1012 - val_loss: 2.2442 - val_accuracy: 0.1043\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3080 - accuracy: 0.0988 - val_loss: 2.2511 - val_accuracy: 0.0999\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.1036 - val_loss: 2.2447 - val_accuracy: 0.1983\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3076 - accuracy: 0.0968 - val_loss: 2.2512 - val_accuracy: 0.1251\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.0970 - val_loss: 2.2435 - val_accuracy: 0.2033\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 2.3093 - accuracy: 0.1004 - val_loss: 2.2487 - val_accuracy: 0.3146\n"
     ]
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    'n_hidden': [0, 1, 2, 3],\n",
    "    'n_neurons': np.arange(1, 100),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(history, param_distribs, n_iter=10, cv=3)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('2. history.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-cdce9363a7f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "history.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
